{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Offene Fragen\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modelpriors: random.randint durch random.choice ersetzt, um Modelpriors spezifizieren zu kÃ¶nnen -> Passt?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.3.0'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.version.VERSION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hierarchical Normal Example Model\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "x_n &\\sim p(x | \\theta_l, \\sigma^2) \\text{ for } n=1,...,N \\text { (assume } \\sigma^2 \\text { to be known (=1))}\\\\\n",
    "\\theta_l &\\sim p(\\theta | \\mu, \\tau^2) \\text{ for } l=1,...,L\\\\\n",
    "\\mu &\\sim p(\\mu | \\mu_0, \\tau_0)\\\\\n",
    "\\tau^2 &\\sim p(\\tau^2 | \\alpha, \\beta)\\\\\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{align}\n",
    "&\\text{Null Model }H_0 \\text{: } \\mu=0\\\\\n",
    "&\\text{Alternative Model }H_1 \\text{: } \\mu \\text{ allowed to differ from 0} \\\\\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simulator class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HierarchicalNormalSimulator:\n",
    "    \n",
    "    def __init__(self):    \n",
    "        pass\n",
    "    \n",
    "    def draw_from_prior(self, model_index, n_clusters, mu0, tau20, alpha, beta):\n",
    "        \"\"\"\n",
    "        Draws parameter values from the specified prior distributions of the \n",
    "        hyperprior and the conditional prior.\n",
    "        ----------\n",
    "        \n",
    "        Arguments:        \n",
    "        n_clusters : int -- number of higher order clusters that the observations are nested in\n",
    "        mu0        : float -- higher order mean prior - mean\n",
    "        tau20      : float -- higher order mean prior - variance\n",
    "        alpha      : float -- higher order variance prior - shape parameter\n",
    "        beta       : float -- higher order variance prior - rate parameter\n",
    "        \"\"\"\n",
    "        \n",
    "        if model_index == 0: \n",
    "            mu = 0\n",
    "        if model_index == 1:\n",
    "            mu = np.random.normal(loc=mu0, scale=np.sqrt(tau20))\n",
    "            \n",
    "        tau2 = 1/np.random.gamma(alpha, beta)\n",
    "        theta = np.random.normal(loc=mu, scale=np.sqrt(tau2), size=n_clusters)\n",
    "        return theta\n",
    "    \n",
    "    def gen_from_likelihood(self, theta, n_obs):\n",
    "        \"\"\"\n",
    "        Generates a single hierarchical dataset from the sampled parameter values.\n",
    "        ----------\n",
    "        \n",
    "        Arguments: \n",
    "        params         : list -- parameters sampled from prior \n",
    "        n_obs : int -- number of observations per cluster\n",
    "        \"\"\"\n",
    "        \n",
    "        X = np.random.normal(loc=theta, scale=1, size=(n_obs, theta.shape[0])).T \n",
    "        return X\n",
    "    \n",
    "    def generate_single(self, model_index, n_clusters, n_obs, mu0=0, tau20=1, alpha=1, beta=1):\n",
    "        \"\"\"\n",
    "        Generates a single hierarchical dataset.\n",
    "        ----------\n",
    "        \n",
    "        Arguments:\n",
    "        model_index    : int -- index of the model to be simulated from\n",
    "        n_clusters     : int -- number of higher order clusters that the observations are nested in\n",
    "        n_obs          : int -- number of observations per cluster\n",
    "        mu0            : float -- higher order mean prior - mean\n",
    "        tau20          : float -- higher order mean prior - variance\n",
    "        alpha          : float -- higher order variance prior - shape parameter\n",
    "        beta           : float -- higher order variance prior - rate parameter\n",
    "        --------\n",
    "        \n",
    "        Returns:\n",
    "        numpy array of shape (n_clusters, n_obs, n_variables) - contains the simulated hierarchical datasets\n",
    "        \"\"\"\n",
    "        prior_sample = self.draw_from_prior(model_index, n_clusters, mu0, tau20, alpha, beta)\n",
    "        x_generated = self.gen_from_likelihood(prior_sample, n_obs)\n",
    "        return x_generated[...,np.newaxis]\n",
    "        \n",
    "    \n",
    "    def simulate(self, batch_size, n_models, n_clusters, n_obs, mu0=0, tau20=1, alpha=1, beta=1):\n",
    "        \"\"\"\n",
    "        Simulates multiple hierarchical datasets. Useful for single usage and debugging (both without the MainSimulator).\n",
    "        ----------\n",
    "        \n",
    "        Arguments:\n",
    "        batch_size     : int -- number of batches to be generated\n",
    "        n_models       : int -- number of models to be simulated from\n",
    "        n_clusters     : int -- number of higher order clusters that the observations are nested in\n",
    "        n_obs          : int -- number of observations per cluster\n",
    "        n_variables    : int -- number of variables in the simulated datasets \n",
    "        mu0            : float -- higher order mean prior - mean\n",
    "        tau20          : float -- higher order mean prior - variance\n",
    "        alpha          : float -- higher order variance prior - shape parameter\n",
    "        beta           : float -- higher order variance prior - rate parameter\n",
    "        --------\n",
    "        \n",
    "        Returns:\n",
    "        numpy array of shape (batch_size * n_models, n_clusters, n_obs, n_variables) - contains the simulated hierarchical datasets\n",
    "        \"\"\"\n",
    "        \n",
    "        X = []\n",
    "        for b in range(batch_size):\n",
    "            prior_sample = self.draw_from_prior(n_clusters, mu0, tau20, alpha, beta)\n",
    "            x_generated = self.gen_from_likelihood(prior_sample, n_obs)\n",
    "            X.append(x_generated)\n",
    "        return np.array(X)[...,np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MainSimulator:\n",
    "    \n",
    "    def __init__(self, simulator):\n",
    "        \n",
    "        self.simulator = simulator\n",
    "    \n",
    "    def draw_from_model_prior(self, batch_size, n_models, model_prior):\n",
    "        \"\"\"\n",
    "        Creates the sequence of models to be simulated from in the batch.\n",
    "        ----------\n",
    "        \n",
    "        Arguments:\n",
    "        batch_size     : int -- number of batches to be generated\n",
    "        n_models       : int -- number of models to be simulated from\n",
    "        model_prior    : list -- prior model probabilities\n",
    "        --------\n",
    "        \n",
    "        Returns:\n",
    "        array of shape (batch_size) - array of indices corresponding to the sampled model from p(M).\n",
    "        \"\"\"\n",
    "        \n",
    "        # create base list of model indices\n",
    "        model_base_indices = [*range(n_models)]\n",
    "        \n",
    "        # uniform prior over model probabilities if no model prior given\n",
    "        if model_prior == None:\n",
    "            model_prior = [1/n_models] * n_models\n",
    "        \n",
    "        # generate sampling list of model indeces\n",
    "        model_indices = np.random.choice(model_base_indices, size=batch_size, p=model_prior)\n",
    "        return model_indices\n",
    "    \n",
    "    def simulate(self, batch_size, n_models, model_prior, n_clust_min=2, n_clust_max=100, n_obs_min=2, n_obs_max=200):\n",
    "        \"\"\"\n",
    "        Simulates a batch of hierarchical datasets.\n",
    "        ----------\n",
    "        \n",
    "        Arguments:\n",
    "        batch_size     : int -- number of datasets to be generated per batch\n",
    "        n_models       : int -- number of models to be simulated from\n",
    "        model_prior    : list -- prior model probabilities\n",
    "        n_clust_min    : int -- minimum number of clusters\n",
    "        n_clust_max    : int -- maximum number of cluster\n",
    "        n_obs_min      : int -- minimum number of observations\n",
    "        n_obs_max      : int -- maximum number of observations\n",
    "        --------\n",
    "        \n",
    "        Returns:\n",
    "        dict of {'X' : array of shape (batch_size, n_clusters, n_obs, n_variables),  \n",
    "                 'm' : array of shape (batch_size)}\n",
    "        \"\"\"\n",
    "        # Draw K and N (equal for all datasets in the batch)\n",
    "        n_clusters = np.random.randint(n_clust_min, n_clust_max+1)\n",
    "        n_obs = np.random.randint(n_obs_min, n_obs_max+1)\n",
    "        \n",
    "        # Draw sampling list of model indices\n",
    "        model_indices = self.draw_from_model_prior(batch_size, n_models, model_prior)\n",
    "        \n",
    "        # Prepare an array to hold simulations\n",
    "        X_gen = np.zeros((batch_size, n_clusters, n_obs, 1), dtype=np.float32)\n",
    "        \n",
    "        for b in range(batch_size):\n",
    "            X_gen[b] = self.simulator.generate_single(model_indices[b], n_clusters, n_obs)\n",
    "               \n",
    "        return {'X': X_gen, 'm': model_indices}\n",
    "    \n",
    "    def __call__(self, batch_size, n_models=2, model_prior=None, n_clust_min=2, n_clust_max=100, n_obs_min=2, n_obs_max=200):\n",
    "        return self.simulate(batch_size, n_models, model_prior, n_clust_min, n_clust_max, n_obs_min, n_obs_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct simulator\n",
    "\n",
    "main_sim = MainSimulator(HierarchicalNormalSimulator())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 63, 1)\n",
      "[30 34]\n",
      "Wall time: 97.7 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Testing zone: how do shapes and index distribution change in different runs?\n",
    "\n",
    "print(main_sim(64)[\"X\"][:, 0, :, :].shape) # shape of 1 cluster \n",
    "print(np.unique(main_sim(64)[\"m\"], return_counts=True)[1]) # model index distribution in batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 69, 49, 1)\n",
      "[[[-2.131488  ]\n",
      "  [-2.1201575 ]\n",
      "  [-3.5995593 ]]\n",
      "\n",
      " [[ 0.07386541]\n",
      "  [-1.8807907 ]\n",
      "  [-0.84533495]]]\n",
      "Wall time: 1.99 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Simulate a batch for subsequent NN development\n",
    "\n",
    "simulated_batch = main_sim(2)\n",
    "print(simulated_batch[\"X\"].shape)\n",
    "print(simulated_batch[\"X\"][0,:2,:3,:]) # display the first 3 observations of the first 2 clusters of the first batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network Class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Structure:\n",
    "- Invariant Layer Class\n",
    "- Equivariant Layer Class\n",
    "- Hierarchical Invariant Network Class (Equivar. + Invar.)\n",
    "- Deep Hierarchical Evidential Model Class (multiple Hier.Inv.Netw.)\n",
    "\n",
    "To do:\n",
    "- Invariant module:\n",
    "    - Learnable Pooling in init() und call()\n",
    "    - call() Description of arguments and return\n",
    "- Equivariant module:\n",
    "    - call() Description of arguments and return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Settings\n",
    "\n",
    "n_dense_inv = 2 # no of layers pre pooling in the invariant network\n",
    "n_dense_post = 2 # no of layers post pooling in the invariant network\n",
    "n_dense_equiv = 2 # no of layers in the equivariant network\n",
    "n_equiv = 2 # no of equivariant modules to be stacked"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Invariant Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InvariantModule(tf.keras.Model):\n",
    "    \"\"\"Implements an invariant nn module as proposed by Bloem-Reddy and Teh (2019).\"\"\"\n",
    "\n",
    "    def __init__(self, n_dense_inv):\n",
    "        \"\"\"\n",
    "        Creates an invariant function with mean pooling.\n",
    "        ----------\n",
    "        \n",
    "        Arguments:\n",
    "        n_dense_inv : int -- no of layers pre pooling in the invariant network\n",
    "        n_dense_post : int -- no of layers post pooling in the invariant network\n",
    "        \"\"\"\n",
    "        \n",
    "        super(InvariantModule, self).__init__()\n",
    "        \n",
    "        # Pre pooling network\n",
    "        self.module = tf.keras.Sequential([tf.keras.layers.Dense(units = 64,\n",
    "                                                                 activation='elu',\n",
    "                                                                 kernel_initializer='glorot_normal')\n",
    "                                           for _ in range(n_dense_inv)])\n",
    "        \n",
    "        # LEARNABLE POOLING?\n",
    "        self.weights_layer = None\n",
    "        \n",
    "        # Post pooling network\n",
    "        self.post_pooling_dense = tf.keras.Sequential([tf.keras.layers.Dense(units = 64,\n",
    "                                                                             activation='elu',\n",
    "                                                                             kernel_initializer='glorot_normal')\n",
    "                                                      for _ in range(n_dense_inv)])\n",
    "        \n",
    "            \n",
    "    def call(self, x):\n",
    "        \"\"\"\n",
    "        Transforms the input into an invariant representation.\n",
    "        ----------\n",
    "        \n",
    "        Arguments:\n",
    "        x : tf.Tensor of shape ???\n",
    "        --------\n",
    "        \n",
    "        Returns:\n",
    "        out: tf.Tensor of shape ???\n",
    "        \"\"\"\n",
    "        \n",
    "        # Embed input\n",
    "        x_emb = self.module(x)\n",
    "        \n",
    "        # COMPUTE WEIGHTS IF EXISTING or perform mean pooling\n",
    "        w_x = tf.reduce_mean(x_emb, axis=-1) # always reduce dimensionality of last axis\n",
    "        \n",
    "        # Increase representational power\n",
    "        out = self.post_pooling_dense(w_x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 69, 49, 1) - Shape of simulated batch of data\n",
      "(2, 69, 64) - Shape of first invariant module output\n",
      "(2, 64) - Shape of second invariant module output\n"
     ]
    }
   ],
   "source": [
    "# Test if dimensionality reduction of invariant module works\n",
    "\n",
    "print('{} - Shape of the simulated batch of data'.format(simulated_batch[\"X\"].shape))\n",
    "\n",
    "inv = InvariantModule(n_dense_inv)\n",
    "reduce_to_3D = inv(x=simulated_batch[\"X\"])\n",
    "print('{} - Shape of the first invariant module output'.format(d_prime.shape))\n",
    "\n",
    "inv = InvariantModule(n_dense_inv) # WARUM MUSS HIER NEU INITIALISIERT WERDEN? reset graph etc reicht nicht, Input size inkompatibel\n",
    "reduce_to_2D = inv(reduce_to_3D)\n",
    "print('{} - Shape of the second invariant module output'.format(reduce_to_2D.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Equivariant Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EquivariantModule(tf.keras.Model):\n",
    "    \"\"\"Implements an equivariant nn module as proposed by Bloem-Reddy and Teh (2019).\"\"\"\n",
    "\n",
    "    def __init__(self, n_dense_equiv, n_dense_inv):\n",
    "        \"\"\"\n",
    "        Creates an equivariant neural network consisting of a FC network with\n",
    "        equal number of hidden units in each layer and an invariant module\n",
    "        with the same FC structure.\n",
    "        ----------\n",
    "        \n",
    "        Arguments:\n",
    "        n_dense_equiv : int -- no of layers in the equivariant network\n",
    "        \"\"\"\n",
    "        \n",
    "        super(EquivariantModule,self).__init__()\n",
    "        \n",
    "        # Equivariant Network\n",
    "        self.module = tf.keras.Sequential([tf.keras.layers.Dense(units = 32,\n",
    "                                                                 activation='elu',\n",
    "                                                                 kernel_initializer='glorot_normal')\n",
    "                                           for _ in range(n_dense_equiv)])\n",
    "        \n",
    "        self.invariant_module = InvariantModule(n_dense_inv)\n",
    "        \n",
    "    def call (self, x):\n",
    "        \"\"\"\n",
    "        Transforms the input into an equivariant representation.\n",
    "        ----------\n",
    "        \n",
    "        Arguments:\n",
    "        x : tf.Tensor of shape ???\n",
    "        --------\n",
    "        \n",
    "        Returns:\n",
    "        out: tf.Tensor of shape ???\n",
    "        \"\"\"\n",
    "        \n",
    "        # Run input through invariant network \n",
    "        x_inv = self.invariant_module(x)\n",
    "        \n",
    "        # Repeat x_inv n times so that it matches x\n",
    "        # read out position -2 so that the layer to be permuted is accessed in 3D as well as 4D\n",
    "        x_inv = tf.stack([x_inv] * int(x.shape[-2]), axis=-2)\n",
    "        \n",
    "        # Concat original input with invariant transformation\n",
    "        x = tf.concat((x_inv, x), axis=-1)\n",
    "        \n",
    "        # Run through equivariant network\n",
    "        out = self.module(x)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 69, 49, 1) - Shape of the simulated batch of data\n",
      "(2, 69, 49, 32) - Shape of the equivariant module output in 4D\n",
      "(2, 69, 32) - Shape of the equivariant module output in 3D\n"
     ]
    }
   ],
   "source": [
    "# Test if equivariant transformation of equivariant module works\n",
    "\n",
    "print('{} - Shape of the simulated batch of data'.format(simulated_batch[\"X\"].shape))\n",
    "\n",
    "# test with 4D data\n",
    "equiv = EquivariantModule(n_dense_equiv, n_dense_inv)\n",
    "x_equiv4D = equiv(x=simulated_batch[\"X\"])\n",
    "print('{} - Shape of the equivariant module output in 4D'.format(x_equiv4D.shape))\n",
    "\n",
    "# test with 3D data\n",
    "equiv = EquivariantModule(n_dense_equiv, n_dense_inv)\n",
    "x_equiv3D = equiv(x=reduce_to_3D)\n",
    "print('{} - Shape of the equivariant module output in 3D'.format(x_equiv3D.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 69, 64) - Shape of the invariant module output when given the 4D-output of the equivariant module\n"
     ]
    }
   ],
   "source": [
    "# Test if the invariant module can handle the output of the equivariant module\n",
    "\n",
    "inv = InvariantModule(n_dense_inv)\n",
    "inv_with_equiv_input = inv(x_equiv4D)\n",
    "print('{} - Shape of the invariant module output when given the 4D-output of the equivariant module'.format(inv_with_equiv_input.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 69, 49, 32) - Shape of the equivariant module output when given the 4D-output of the equivariant module\n"
     ]
    }
   ],
   "source": [
    "# Test if multiple equivariant modules are stackable\n",
    "\n",
    "equiv = EquivariantModule(n_dense_equiv, n_dense_inv)\n",
    "equiv1 = equiv(x=x_equiv4D)\n",
    "print('{} - Shape of the equivariant module output when given the 4D-output of the equivariant module'.format(x_equiv4D.shape))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tf] *",
   "language": "python",
   "name": "conda-env-tf-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
