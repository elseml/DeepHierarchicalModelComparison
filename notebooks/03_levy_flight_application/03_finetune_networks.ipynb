{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "sys.path.append(os.path.abspath(os.path.join('../..'))) # access sibling directories\n",
    "sys.path.append(\"C:\\\\Users\\\\lasse\\\\Documents\\\\GitHub\\\\BayesFlow\")\n",
    "\n",
    "from src.python.settings import summary_meta_diffusion, evidence_meta_diffusion\n",
    "from src.python.helpers import load_simulated_rt_data, mask_inputs\n",
    "from src.python.networks import HierarchicalInvariantNetwork, EvidentialNetwork\n",
    "\n",
    "import numpy as np\n",
    "from tensorflow.keras.experimental import CosineDecayRestarts\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from functools import partial\n",
    "\n",
    "from bayesflow.trainers import ModelComparisonTrainer\n",
    "from bayesflow.amortizers import MultiModelAmortizer \n",
    "from bayesflow.losses import log_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tuning with 900 trials per person"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "levy_sims_folder = 'c:\\\\Users\\\\lasse\\\\documents\\\\hierarchical model comparison project\\\\data\\\\Levy_flight_application'\n",
    "\n",
    "indices_900_filename = \"train_indices_900_trials.npy\"\n",
    "datasets_900_filename = \"train_datasets_900_trials.npy\"\n",
    "\n",
    "indices_900, datasets_900 = load_simulated_rt_data(levy_sims_folder, indices_900_filename, datasets_900_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When only conducting fine-tuning: manually move/delete fine-tuning checkpoints in checkpoints folder so that training resumes from pretrained network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINER INITIALIZATION: No generative model provided. Only offline learning mode is available!\n",
      "Networks loaded from c:\\Users\\lasse\\Dropbox\\Uni Mannheim\\M.Sc\\4. Semester\\Masterarbeit\\Coding\\notebooks\\03_levy_flight_application\\training_checkpoints\\trial_4\\ckpt-57\n"
     ]
    }
   ],
   "source": [
    "summary_net = HierarchicalInvariantNetwork(summary_meta_diffusion)\n",
    "evidence_net = EvidentialNetwork(evidence_meta_diffusion)\n",
    "amortizer = MultiModelAmortizer(evidence_net, summary_net)\n",
    "\n",
    "# Cosine Decay with Restarts\n",
    "initial_lr = 0.00005 # Shrink LR by factor 10 for fine-tuning\n",
    "first_decay_steps = 250\n",
    "t_mul = 2\n",
    "m_mul = 0.9\n",
    "alpha = 0.2\n",
    "lr_schedule_restart = CosineDecayRestarts(\n",
    "    initial_lr, first_decay_steps, t_mul=t_mul, m_mul=m_mul, alpha=alpha)\n",
    "\n",
    "# Checkpoint path for loading pretrained network and saving the final network\n",
    "trial_folder = 'trial_4'\n",
    "checkpoint_path = os.path.join(os.getcwd(), 'training_checkpoints', trial_folder)\n",
    "#checkpoint_path = os.path.join(os.getcwd(), 'checkpoints', trial_folder) # rename checkpoint folder to solve the odd \"UnknownError: Failed to rename\" error that can happen during training\n",
    "\n",
    "trainer = ModelComparisonTrainer(\n",
    "    network=amortizer, \n",
    "    loss=partial(log_loss, kl_weight=0.25),\n",
    "    optimizer=partial(Adam, lr_schedule_restart),\n",
    "    checkpoint_path=checkpoint_path,\n",
    "    skip_checks=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting 8000 simulations to a TensorFlow data set...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5bcba9c95a44328adb6d6ed071945dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training epoch 1:   0%|          | 0/250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 finished.\n",
      "Converting 8000 simulations to a TensorFlow data set...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57afa88eee634e639bf238331b41e27b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training epoch 1:   0%|          | 0/250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 finished.\n",
      "Converting 8000 simulations to a TensorFlow data set...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c092175490ae45e89d5de3fdd39d081f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training epoch 1:   0%|          | 0/250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 2 finished.\n",
      "Converting 8000 simulations to a TensorFlow data set...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a28644375f0b4120b303bf895e87a2a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training epoch 1:   0%|          | 0/250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3 finished.\n",
      "Converting 8000 simulations to a TensorFlow data set...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "001a4c3dffe54476a06f671c41a7ed02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training epoch 1:   0%|          | 0/250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 4 finished.\n",
      "Converting 8000 simulations to a TensorFlow data set...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff610d0966564b9b9852f617a6564311",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training epoch 1:   0%|          | 0/250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 5 finished.\n",
      "Converting 8000 simulations to a TensorFlow data set...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79cc97276c1b4bb78dcd886139ed937f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training epoch 1:   0%|          | 0/250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 6 finished.\n"
     ]
    }
   ],
   "source": [
    "# Mask some training data so that training leads to a robust net that can handle missing data\n",
    "epochs = 7\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    datasets_900_masked = mask_inputs(datasets_900, missings_mean=28.5, missings_sd=13.5, missing_rts_equal_mean=True)\n",
    "    losses = trainer.train_offline(epochs=1, batch_size=32, \n",
    "                               model_indices=indices_900, sim_data=datasets_900_masked)\n",
    "    print(f\"epoch {epoch} finished.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
