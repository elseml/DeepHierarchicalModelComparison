import tensorflow as tf



def softmax_loss(network, model_indices, sim_data):
    """ Computes the logloss given softmax probs and true model indices m_true.
        Requires the model probability network to possess a softmax output layer.
    Parameters
    ----------
    network       : tf.keras.Model
        A model probability with an optional summary network
    model_indices : tf.Tensor of shape (batch_size, n_models)
        True model indices
    sim_data      : tf.Tensor of shape (batch_size, n_obs, data_dim) or (batch_size, summary_dim) 
        Synthetic data sets generated by the params or summary statistics thereof
    Returns
    -------
    loss : tf.Tensor
        A single scalar Monte-Carlo approximation of the regularized Bayes risk, shape (,)
    """

    # Compute evidences and softmax probabilities
    softmax_probs = network(sim_data)

    # Numerical stability
    softmax_probs = tf.clip_by_value(softmax_probs, 1e-15, 1 - 1e-15)
    
    # Compute softmax loss
    cat_cross_entropy = tf.keras.losses.CategoricalCrossentropy()
    softmax_loss = cat_cross_entropy(model_indices, softmax_probs)

    # Combine losses 
    loss = softmax_loss

    return loss